
## Этап 5. Асинхронное взаимодействие (bonus deadline 2022-11-02 23:59:59 MSK, hard deadline 2022-11-09 23:59:59 MSK)

Переключаем внутреннее взаимодействие узлов на асинхронный `java.net.http.HttpClient`.
**Параллельно** отправляем запросы репликам и собираем **самые быстрые** ответы на `CompletableFuture`.

Проведите нагрузочное тестирование с помощью [wrk2](https://github.com/giltene/wrk2) в несколько соединений.

Отпрофилируйте приложение (CPU, alloc и **lock**) под нагрузкой и сравните результаты latency и профилирования по сравнению с неасинхронной версией.

Присылайте pull request со своей реализацией на review.

# Отчет 

На предыдущем этапе я попытался улучшить производительность сервиса добавив асинхронности.
Я завел executor service и сначала отправил все запросы асинхронно, а потом по очереди их ждал через future.get().

### PUT

Rate = 6000

```
Running 1m test @ http://localhost:42342
  6 threads and 64 connections
  Thread calibration: mean lat.: 121.752ms, rate sampling interval: 453ms
  Thread calibration: mean lat.: 124.892ms, rate sampling interval: 464ms
  Thread calibration: mean lat.: 121.629ms, rate sampling interval: 454ms
  Thread calibration: mean lat.: 121.998ms, rate sampling interval: 450ms
  Thread calibration: mean lat.: 122.751ms, rate sampling interval: 458ms
  Thread calibration: mean lat.: 122.008ms, rate sampling interval: 454ms
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency    63.41ms  139.96ms 996.86ms   95.08%
    Req/Sec     0.98k    77.55     1.10k    86.13%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%   16.14ms
 75.000%   69.06ms
 90.000%  127.17ms
 99.000%  921.09ms
 99.900%  984.06ms
 99.990%  992.77ms
 99.999%  997.38ms
100.000%  997.38ms

  Detailed Percentile spectrum:
       Value   Percentile   TotalCount 1/(1-Percentile)

       0.327     0.000000            1         1.00
       2.897     0.100000        29432         1.11
       4.223     0.200000        58821         1.25
       6.367     0.300000        88231         1.43
      10.495     0.400000       117678         1.67
      16.143     0.500000       147059         2.00
      21.551     0.550000       161751         2.22
      35.071     0.600000       176451         2.50
      47.167     0.650000       191164         2.86
      58.015     0.700000       205892         3.33
      69.055     0.750000       220659         4.00
      73.791     0.775000       227931         4.44
      79.039     0.800000       235286         5.00
      87.039     0.825000       242627         5.71
      98.687     0.850000       249979         6.67
     110.015     0.875000       257341         8.00
     117.759     0.887500       261001         8.89
     127.167     0.900000       264687        10.00
     135.935     0.912500       268366        11.43
     142.335     0.925000       272043        13.33
     149.119     0.937500       275745        16.00
     155.263     0.943750       277548        17.78
     179.071     0.950000       279382        20.00
     290.047     0.956250       281227        22.86
     314.623     0.962500       283069        26.67
     347.903     0.968750       284893        32.00
     393.215     0.971875       285813        35.56
     551.935     0.975000       286734        40.00
     655.359     0.978125       287654        45.71
     728.063     0.981250       288571        53.33
     816.639     0.984375       289491        64.00
     841.727     0.985938       289951        71.11
     870.911     0.987500       290407        80.00
     904.703     0.989062       290874        91.43
     929.791     0.990625       291328       106.67
     944.639     0.992188       291798       128.00
     949.759     0.992969       292046       142.22
     953.855     0.993750       292268       160.00
     957.439     0.994531       292482       182.86
     961.535     0.995313       292714       213.33
     966.655     0.996094       292943       256.00
     969.727     0.996484       293072       284.44
     971.775     0.996875       293181       320.00
     974.335     0.997266       293295       365.71
     976.895     0.997656       293414       426.67
     978.431     0.998047       293525       512.00
     979.455     0.998242       293589       568.89
     980.479     0.998437       293644       640.00
     981.503     0.998633       293689       731.43
     983.039     0.998828       293760       853.33
     984.575     0.999023       293810      1024.00
     985.087     0.999121       293838      1137.78
     985.599     0.999219       293856      1280.00
     986.111     0.999316       293886      1462.86
     987.135     0.999414       293926      1706.67
     987.647     0.999512       293950      2048.00
     988.159     0.999561       293969      2275.56
     988.159     0.999609       293969      2560.00
     988.671     0.999658       293983      2925.71
     989.695     0.999707       294013      3413.33
     989.695     0.999756       294013      4096.00
     990.207     0.999780       294026      4551.11
     990.207     0.999805       294026      5120.00
     991.231     0.999829       294039      5851.43
     991.743     0.999854       294048      6826.67
     991.743     0.999878       294048      8192.00
     992.255     0.999890       294053      9102.22
     992.767     0.999902       294060     10240.00
     992.767     0.999915       294060     11702.86
     993.279     0.999927       294062     13653.33
     993.791     0.999939       294067     16384.00
     993.791     0.999945       294067     18204.44
     994.303     0.999951       294072     20480.00
     994.303     0.999957       294072     23405.71
     994.815     0.999963       294074     27306.67
     995.327     0.999969       294075     32768.00
     995.327     0.999973       294075     36408.89
     995.839     0.999976       294077     40960.00
     995.839     0.999979       294077     46811.43
     996.351     0.999982       294079     54613.33
     996.351     0.999985       294079     65536.00
     996.351     0.999986       294079     72817.78
     997.375     0.999988       294083     81920.00
     997.375     1.000000       294083          inf
#[Mean    =       63.407, StdDeviation   =      139.963]
#[Max     =      996.864, Total count    =       294083]
#[Buckets =           27, SubBuckets     =         2048]
----------------------------------------------------------
  354019 requests in 1.00m, 22.62MB read
Requests/sec:   5900.29
Transfer/sec:    386.05KB
```

Видим, что получившаяся реализация работает медленнее - была надежда на то, что если отправить запросы параллельно,
то они успеют обработаться до того как мы начнем их собирать на future.get(), однако мы добавили слишком много накладных
ресурсов из-за нового ExecutorService, а также основное узкое место Coordinator, который ждет ответа от других кластеров
никуда не делось.

### CPU
![img_6.png](img_6.png)

### ALLOC

![img_8.png](img_8.png)

### LOCK

![img_7.png](img_7.png)

Видим, что ThreadPoolExecutor стал использовать еще больше ресурсов, а также часть вызовов переместилось из ServiceImpl.handle
в future.

Я сделал вывод: \
Исходя из результатов понятно, что если данная оптимизация если и работает то точно не на одной машине, так как
в данном случае мы добавили накладных ресурсов, не уменьшив кол-во работы. Нужна большая параллельность ресурсов, чтобы
формула: Быстрее отправил - быстрее получил ответы сработала. Нужна более эффективная реализация асинхронного клиента.


После этого я посмотрел лекцию [Сергея Куксенко](https://www.youtube.com/watch?v=W7iK74YA5NM) и решил для асинхронного 
клиента использовать Completable Future.

### PUT

Посмотрим справится ли новая реализация c Rate=10000

```
Running 1m test @ http://localhost:42342
  6 threads and 64 connections
  Thread calibration: mean lat.: 2435.799ms, rate sampling interval: 9404ms
  Thread calibration: mean lat.: 2477.851ms, rate sampling interval: 8912ms
  Thread calibration: mean lat.: 2433.714ms, rate sampling interval: 9379ms
  Thread calibration: mean lat.: 2421.178ms, rate sampling interval: 9338ms
  Thread calibration: mean lat.: 2427.539ms, rate sampling interval: 9396ms
  Thread calibration: mean lat.: 2437.763ms, rate sampling interval: 9371ms
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency    18.44s     7.86s   32.28s    57.26%
    Req/Sec   774.40     29.48   808.00     66.67%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%   18.66s 
 75.000%   25.33s 
 90.000%   29.26s 
 99.000%   31.65s 
 99.900%   32.01s 
 99.990%   32.18s 
 99.999%   32.28s 
100.000%   32.29s 

  Detailed Percentile spectrum:
       Value   Percentile   TotalCount 1/(1-Percentile)

    4857.855     0.000000            1         1.00
    7585.791     0.100000        23234         1.11
   10166.271     0.200000        46452         1.25
   12754.943     0.300000        69652         1.43
   15417.343     0.400000        92874         1.67
   18661.375     0.500000       116198         2.00
   20054.015     0.550000       127739         2.22
   21364.735     0.600000       139394         2.50
   22642.687     0.650000       151039         2.86
   24018.943     0.700000       162539         3.33
   25329.663     0.750000       174165         4.00
   25935.871     0.775000       179931         4.44
   26640.383     0.800000       185809         5.00
   27279.359     0.825000       191597         5.71
   27951.103     0.850000       197392         6.67
   28606.463     0.875000       203173         8.00
   28917.759     0.887500       206094         8.89
   29261.823     0.900000       208973        10.00
   29605.887     0.912500       211900        11.43
   29933.567     0.925000       214753        13.33
   30261.247     0.937500       217741        16.00
   30425.087     0.943750       219193        17.78
   30588.927     0.950000       220644        20.00
   30752.767     0.956250       222043        22.86
   30916.607     0.962500       223509        26.67
   31080.447     0.968750       224957        32.00
   31162.367     0.971875       225684        35.56
   31244.287     0.975000       226394        40.00
   31326.207     0.978125       227083        45.71
   31408.127     0.981250       227822        53.33
   31490.047     0.984375       228536        64.00
   31539.199     0.985938       228972        71.11
   31588.351     0.987500       229399        80.00
   31621.119     0.989062       229680        91.43
   31670.271     0.990625       230059       106.67
   31719.423     0.992188       230397       128.00
   31752.191     0.992969       230620       142.22
   31768.575     0.993750       230729       160.00
   31801.343     0.994531       230960       182.86
   31834.111     0.995313       231166       213.33
   31850.495     0.996094       231255       256.00
   31866.879     0.996484       231348       284.44
   31883.263     0.996875       231442       320.00
   31916.031     0.997266       231596       365.71
   31932.415     0.997656       231660       426.67
   31948.799     0.998047       231731       512.00
   31965.183     0.998242       231796       568.89
   31981.567     0.998437       231851       640.00
   31981.567     0.998633       231851       731.43
   31997.951     0.998828       231907       853.33
   32014.335     0.999023       231957      1024.00
   32014.335     0.999121       231957      1137.78
   32030.719     0.999219       231998      1280.00
   32047.103     0.999316       232031      1462.86
   32047.103     0.999414       232031      1706.67
   32063.487     0.999512       232064      2048.00
   32063.487     0.999561       232064      2275.56
   32079.871     0.999609       232086      2560.00
   32079.871     0.999658       232086      2925.71
   32096.255     0.999707       232101      3413.33
   32112.639     0.999756       232116      4096.00
   32112.639     0.999780       232116      4551.11
   32112.639     0.999805       232116      5120.00
   32129.023     0.999829       232124      5851.43
   32145.407     0.999854       232133      6826.67
   32145.407     0.999878       232133      8192.00
   32161.791     0.999890       232137      9102.22
   32178.175     0.999902       232141     10240.00
   32194.559     0.999915       232145     11702.86
   32194.559     0.999927       232145     13653.33
   32210.943     0.999939       232148     16384.00
   32227.327     0.999945       232152     18204.44
   32227.327     0.999951       232152     20480.00
   32227.327     0.999957       232152     23405.71
   32243.711     0.999963       232154     27306.67
   32243.711     0.999969       232154     32768.00
   32260.095     0.999973       232157     36408.89
   32260.095     0.999976       232157     40960.00
   32260.095     0.999979       232157     46811.43
   32260.095     0.999982       232157     54613.33
   32276.479     0.999985       232159     65536.00
   32276.479     0.999986       232159     72817.78
   32276.479     0.999988       232159     81920.00
   32276.479     0.999989       232159     93622.86
   32276.479     0.999991       232159    109226.67
   32292.863     0.999992       232161    131072.00
   32292.863     1.000000       232161          inf
#[Mean    =    18442.221, StdDeviation   =     7861.545]
#[Max     =    32276.480, Total count    =       232161]
#[Buckets =           27, SubBuckets     =         2048]
----------------------------------------------------------
  278909 requests in 1.00m, 17.82MB read
Requests/sec:   4648.32
Transfer/sec:    304.14KB
```

Сервис, так же как и на предыдущей итерации, не справился с нагрузкой. \

До добавления асинхронности
![img_1.png](img_1.png)

После добавления асинхронности
![img.png](img.png)

Что интересно при такой же нагрузке на предыдущем этапе сервис потратил 82% CPU в ThreadPool,
а сейчас он потратил 26% на ForkJoinPool и 60% на ThreadPool. Т.е на 4% выросла работа на ThreadPools.
Далее на этапе профилирования разберемся с чем это связано и за счет чего произошел такой рост.

Далее получилось выяснить, что сервис стабильно выдерживает нагрузку при Rate=5000.

```
    Running 1m test @ http://localhost:42342
  6 threads and 64 connections
  Thread calibration: mean lat.: 4.080ms, rate sampling interval: 19ms
  Thread calibration: mean lat.: 4.047ms, rate sampling interval: 19ms
  Thread calibration: mean lat.: 4.149ms, rate sampling interval: 19ms
  Thread calibration: mean lat.: 4.124ms, rate sampling interval: 19ms
  Thread calibration: mean lat.: 4.188ms, rate sampling interval: 19ms
  Thread calibration: mean lat.: 4.105ms, rate sampling interval: 19ms
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     6.42ms   14.96ms 169.98ms   93.85%
    Req/Sec     0.86k    96.79     1.39k    83.07%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%    2.10ms
 75.000%    3.61ms
 90.000%   11.89ms
 99.000%   88.89ms
 99.900%  130.49ms
 99.990%  152.32ms
 99.999%  167.04ms
100.000%  170.11ms

  Detailed Percentile spectrum:
       Value   Percentile   TotalCount 1/(1-Percentile)

       0.345     0.000000            1         1.00
       1.132     0.100000        24998         1.11
       1.399     0.200000        49956         1.25
       1.626     0.300000        74990         1.43
       1.849     0.400000        99984         1.67
       2.103     0.500000       124978         2.00
       2.249     0.550000       137422         2.22
       2.423     0.600000       149846         2.50
       2.645     0.650000       162354         2.86
       2.987     0.700000       174877         3.33
       3.613     0.750000       187311         4.00
       4.203     0.775000       193554         4.44
       5.083     0.800000       199804         5.00
       6.223     0.825000       206060         5.71
       7.615     0.850000       212283         6.67
       9.351     0.875000       218523         8.00
      10.479     0.887500       221661         8.89
      11.887     0.900000       224774        10.00
      13.735     0.912500       227899        11.43
      16.495     0.925000       231016        13.33
      20.927     0.937500       234139        16.00
      23.903     0.943750       235694        17.78
      27.775     0.950000       237254        20.00
      32.543     0.956250       238815        22.86
      38.335     0.962500       240383        26.67
      45.695     0.968750       241937        32.00
      49.887     0.971875       242722        35.56
      55.199     0.975000       243499        40.00
      61.759     0.978125       244277        45.71
      69.631     0.981250       245061        53.33
      76.287     0.984375       245843        64.00
      79.551     0.985938       246233        71.11
      82.815     0.987500       246620        80.00
      86.463     0.989062       247010        91.43
      90.495     0.990625       247400       106.67
      94.783     0.992188       247795       128.00
      96.831     0.992969       247985       142.22
      99.455     0.993750       248181       160.00
     102.143     0.994531       248378       182.86
     105.663     0.995313       248571       213.33
     109.119     0.996094       248766       256.00
     111.295     0.996484       248866       284.44
     113.215     0.996875       248963       320.00
     115.391     0.997266       249060       365.71
     117.887     0.997656       249155       426.67
     121.023     0.998047       249257       512.00
     122.367     0.998242       249304       568.89
     124.095     0.998437       249350       640.00
     126.079     0.998633       249399       731.43
     128.575     0.998828       249448       853.33
     130.687     0.999023       249497      1024.00
     132.223     0.999121       249522      1137.78
     133.887     0.999219       249546      1280.00
     135.935     0.999316       249570      1462.86
     137.855     0.999414       249594      1706.67
     139.647     0.999512       249621      2048.00
     140.671     0.999561       249632      2275.56
     141.823     0.999609       249643      2560.00
     143.359     0.999658       249656      2925.71
     143.999     0.999707       249669      3413.33
     145.279     0.999756       249680      4096.00
     146.175     0.999780       249686      4551.11
     146.943     0.999805       249693      5120.00
     148.095     0.999829       249698      5851.43
     148.607     0.999854       249704      6826.67
     150.911     0.999878       249710      8192.00
     152.063     0.999890       249713      9102.22
     152.831     0.999902       249716     10240.00
     153.215     0.999915       249719     11702.86
     154.495     0.999927       249722     13653.33
     155.647     0.999939       249725     16384.00
     156.287     0.999945       249727     18204.44
     156.543     0.999951       249728     20480.00
     157.567     0.999957       249730     23405.71
     158.975     0.999963       249731     27306.67
     159.231     0.999969       249733     32768.00
     159.743     0.999973       249734     36408.89
     159.743     0.999976       249734     40960.00
     162.175     0.999979       249735     46811.43
     163.583     0.999982       249736     54613.33
     164.351     0.999985       249737     65536.00
     164.351     0.999986       249737     72817.78
     164.351     0.999988       249737     81920.00
     167.039     0.999989       249738     93622.86
     167.039     0.999991       249738    109226.67
     167.295     0.999992       249739    131072.00
     167.295     0.999993       249739    145635.56
     167.295     0.999994       249739    163840.00
     167.295     0.999995       249739    187245.71
     167.295     0.999995       249739    218453.33
     170.111     0.999996       249740    262144.00
     170.111     1.000000       249740          inf
#[Mean    =        6.416, StdDeviation   =       14.957]
#[Max     =      169.984, Total count    =       249740]
#[Buckets =           27, SubBuckets     =         2048]
----------------------------------------------------------
  299906 requests in 1.00m, 19.16MB read
Requests/sec:   4998.48
Transfer/sec:    327.05KB
```

Как мы видим стабильно сервис теперь выдерживает нагрузку 5000. Однако На предыдущем этапе 
сервис выдерживал нагрузку в 6000. \
С чем же это связано? \
Мое предположение состоит в следующем: \
Запуская асинхронный клиент, мы оптимизируем работу по ожиданию ответов от других кластеров. \
Однако запуск производится, как и на предыдущей итерации на 3 кластерах(т.к при большем кол-ве сильно повышается конкуренция
за ресурсы между instancами сервиса). \
Получается мы тратим дополнительные ресурсы на ThreadJoinPool, на дополнительные блокировки, создание фьючей.
